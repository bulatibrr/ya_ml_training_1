{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №7\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:39:04.996276Z",
     "start_time": "2025-02-25T08:39:03.150771Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №1: \n",
    "Обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша основная задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку первого занятия.\n",
    "\n",
    "Настоятельно рекомендуем написать код \"с нуля\", лишь поглядывая на готовые примеры, а не просто \"скопировать-вставить\". Это поможет вам в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:39:33.476121Z",
     "start_time": "2025-02-25T08:39:10.648438Z"
    }
   },
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_mnist_data = MNIST('.', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_mnist_data = MNIST('.', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f'Image label: {_label}')\n",
    "# __________end of block__________"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:12<00:00, 766kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 212kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:02<00:00, 815kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.57MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 92% accuracy.\n",
    "\n",
    "*Комментарий: для этого достаточно линейных слоев и функций активации.*\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model`.__"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:20:23.489417Z",
     "start_time": "2025-02-25T09:20:23.484418Z"
    }
   },
   "source": [
    "class SomeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 392),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(392, 196),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(196, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SomeNet()"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:20:24.621231Z",
     "start_time": "2025-02-25T09:20:24.617509Z"
    }
   },
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model is not None, 'Please, use `model` variable to store your model'\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].reshape(-1, 784)\n",
    "    y = random_batch[1]\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model(x)    \n",
    "except Exception as e:\n",
    "    print('Something is wrong with the model')\n",
    "    raise e\n",
    "    \n",
    "    \n",
    "assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
    "\n",
    "print('Everything seems fine!')\n",
    "# __________end of block__________"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройте параметры модели на обучающей выборке. Рекомендуем поработать с различными оптимизаторами."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:29:24.172347Z",
     "start_time": "2025-02-25T09:28:25.897004Z"
    }
   },
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X.reshape(-1, 784))\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * 32 + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.reshape(-1, 784))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "lfn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_data_loader, model, lfn, optim)\n",
    "    test_loop(test_data_loader, model, lfn)\n",
    "print(\"Done!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.052086  [   32/60000]\n",
      "loss: 0.433884  [ 3232/60000]\n",
      "loss: 0.356380  [ 6432/60000]\n",
      "loss: 0.141056  [ 9632/60000]\n",
      "loss: 0.477433  [12832/60000]\n",
      "loss: 0.469815  [16032/60000]\n",
      "loss: 0.212249  [19232/60000]\n",
      "loss: 0.817154  [22432/60000]\n",
      "loss: 0.588426  [25632/60000]\n",
      "loss: 0.554062  [28832/60000]\n",
      "loss: 0.160309  [32032/60000]\n",
      "loss: 0.145993  [35232/60000]\n",
      "loss: 0.194331  [38432/60000]\n",
      "loss: 0.507368  [41632/60000]\n",
      "loss: 0.138351  [44832/60000]\n",
      "loss: 0.196249  [48032/60000]\n",
      "loss: 0.518435  [51232/60000]\n",
      "loss: 0.108088  [54432/60000]\n",
      "loss: 0.229942  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.292127 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.493173  [   32/60000]\n",
      "loss: 0.337595  [ 3232/60000]\n",
      "loss: 0.548337  [ 6432/60000]\n",
      "loss: 0.430503  [ 9632/60000]\n",
      "loss: 0.381133  [12832/60000]\n",
      "loss: 0.272339  [16032/60000]\n",
      "loss: 0.323367  [19232/60000]\n",
      "loss: 0.186631  [22432/60000]\n",
      "loss: 0.357175  [25632/60000]\n",
      "loss: 0.457204  [28832/60000]\n",
      "loss: 0.115430  [32032/60000]\n",
      "loss: 0.098550  [35232/60000]\n",
      "loss: 0.375704  [38432/60000]\n",
      "loss: 0.287787  [41632/60000]\n",
      "loss: 0.204701  [44832/60000]\n",
      "loss: 0.209382  [48032/60000]\n",
      "loss: 0.467012  [51232/60000]\n",
      "loss: 0.269664  [54432/60000]\n",
      "loss: 0.218936  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.286907 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.200834  [   32/60000]\n",
      "loss: 0.470928  [ 3232/60000]\n",
      "loss: 0.189909  [ 6432/60000]\n",
      "loss: 0.112171  [ 9632/60000]\n",
      "loss: 0.269738  [12832/60000]\n",
      "loss: 0.562840  [16032/60000]\n",
      "loss: 0.133518  [19232/60000]\n",
      "loss: 0.562373  [22432/60000]\n",
      "loss: 0.177197  [25632/60000]\n",
      "loss: 0.133264  [28832/60000]\n",
      "loss: 0.444174  [32032/60000]\n",
      "loss: 0.238752  [35232/60000]\n",
      "loss: 0.271442  [38432/60000]\n",
      "loss: 0.243686  [41632/60000]\n",
      "loss: 0.141274  [44832/60000]\n",
      "loss: 0.122014  [48032/60000]\n",
      "loss: 0.074012  [51232/60000]\n",
      "loss: 0.190120  [54432/60000]\n",
      "loss: 0.145959  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.282855 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.313098  [   32/60000]\n",
      "loss: 0.338933  [ 3232/60000]\n",
      "loss: 0.173537  [ 6432/60000]\n",
      "loss: 0.260022  [ 9632/60000]\n",
      "loss: 0.398015  [12832/60000]\n",
      "loss: 0.203119  [16032/60000]\n",
      "loss: 0.232751  [19232/60000]\n",
      "loss: 0.422431  [22432/60000]\n",
      "loss: 0.350881  [25632/60000]\n",
      "loss: 0.236527  [28832/60000]\n",
      "loss: 0.326348  [32032/60000]\n",
      "loss: 0.457127  [35232/60000]\n",
      "loss: 0.334778  [38432/60000]\n",
      "loss: 0.402997  [41632/60000]\n",
      "loss: 0.435016  [44832/60000]\n",
      "loss: 0.161661  [48032/60000]\n",
      "loss: 0.370674  [51232/60000]\n",
      "loss: 0.462845  [54432/60000]\n",
      "loss: 0.142685  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.278129 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.331893  [   32/60000]\n",
      "loss: 0.203843  [ 3232/60000]\n",
      "loss: 0.302617  [ 6432/60000]\n",
      "loss: 0.287304  [ 9632/60000]\n",
      "loss: 0.310749  [12832/60000]\n",
      "loss: 0.217411  [16032/60000]\n",
      "loss: 0.270519  [19232/60000]\n",
      "loss: 0.248623  [22432/60000]\n",
      "loss: 0.147045  [25632/60000]\n",
      "loss: 0.151516  [28832/60000]\n",
      "loss: 0.453408  [32032/60000]\n",
      "loss: 0.346591  [35232/60000]\n",
      "loss: 0.155050  [38432/60000]\n",
      "loss: 0.372806  [41632/60000]\n",
      "loss: 0.282323  [44832/60000]\n",
      "loss: 0.210123  [48032/60000]\n",
      "loss: 0.325315  [51232/60000]\n",
      "loss: 0.144721  [54432/60000]\n",
      "loss: 0.171683  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.273871 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.297488  [   32/60000]\n",
      "loss: 0.216468  [ 3232/60000]\n",
      "loss: 0.562557  [ 6432/60000]\n",
      "loss: 0.390793  [ 9632/60000]\n",
      "loss: 0.238088  [12832/60000]\n",
      "loss: 0.146880  [16032/60000]\n",
      "loss: 0.199208  [19232/60000]\n",
      "loss: 0.436104  [22432/60000]\n",
      "loss: 0.262254  [25632/60000]\n",
      "loss: 0.068413  [28832/60000]\n",
      "loss: 0.172485  [32032/60000]\n",
      "loss: 0.239790  [35232/60000]\n",
      "loss: 0.183278  [38432/60000]\n",
      "loss: 0.276455  [41632/60000]\n",
      "loss: 0.160706  [44832/60000]\n",
      "loss: 0.436583  [48032/60000]\n",
      "loss: 0.383230  [51232/60000]\n",
      "loss: 0.332613  [54432/60000]\n",
      "loss: 0.277726  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.269827 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.559779  [   32/60000]\n",
      "loss: 0.223400  [ 3232/60000]\n",
      "loss: 0.426110  [ 6432/60000]\n",
      "loss: 0.333015  [ 9632/60000]\n",
      "loss: 0.364087  [12832/60000]\n",
      "loss: 0.202322  [16032/60000]\n",
      "loss: 0.284370  [19232/60000]\n",
      "loss: 0.353457  [22432/60000]\n",
      "loss: 0.304186  [25632/60000]\n",
      "loss: 0.101896  [28832/60000]\n",
      "loss: 0.301761  [32032/60000]\n",
      "loss: 0.365214  [35232/60000]\n",
      "loss: 0.326569  [38432/60000]\n",
      "loss: 0.224312  [41632/60000]\n",
      "loss: 0.451919  [44832/60000]\n",
      "loss: 0.254567  [48032/60000]\n",
      "loss: 0.165677  [51232/60000]\n",
      "loss: 0.258370  [54432/60000]\n",
      "loss: 0.337955  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.265816 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.130866  [   32/60000]\n",
      "loss: 0.144304  [ 3232/60000]\n",
      "loss: 0.327124  [ 6432/60000]\n",
      "loss: 0.298132  [ 9632/60000]\n",
      "loss: 0.143876  [12832/60000]\n",
      "loss: 0.438858  [16032/60000]\n",
      "loss: 0.351289  [19232/60000]\n",
      "loss: 0.166466  [22432/60000]\n",
      "loss: 0.395716  [25632/60000]\n",
      "loss: 0.361502  [28832/60000]\n",
      "loss: 0.187308  [32032/60000]\n",
      "loss: 0.413824  [35232/60000]\n",
      "loss: 0.222531  [38432/60000]\n",
      "loss: 0.192947  [41632/60000]\n",
      "loss: 0.255578  [44832/60000]\n",
      "loss: 0.270831  [48032/60000]\n",
      "loss: 0.368108  [51232/60000]\n",
      "loss: 0.191662  [54432/60000]\n",
      "loss: 0.475468  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.261837 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.251885  [   32/60000]\n",
      "loss: 0.121489  [ 3232/60000]\n",
      "loss: 0.124551  [ 6432/60000]\n",
      "loss: 0.339288  [ 9632/60000]\n",
      "loss: 0.417481  [12832/60000]\n",
      "loss: 0.409467  [16032/60000]\n",
      "loss: 0.251586  [19232/60000]\n",
      "loss: 0.277542  [22432/60000]\n",
      "loss: 0.207974  [25632/60000]\n",
      "loss: 0.098483  [28832/60000]\n",
      "loss: 0.124938  [32032/60000]\n",
      "loss: 0.176987  [35232/60000]\n",
      "loss: 0.188315  [38432/60000]\n",
      "loss: 0.305547  [41632/60000]\n",
      "loss: 0.231795  [44832/60000]\n",
      "loss: 0.231767  [48032/60000]\n",
      "loss: 0.284291  [51232/60000]\n",
      "loss: 0.184126  [54432/60000]\n",
      "loss: 0.042417  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.258412 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.237671  [   32/60000]\n",
      "loss: 0.352887  [ 3232/60000]\n",
      "loss: 0.147545  [ 6432/60000]\n",
      "loss: 0.437296  [ 9632/60000]\n",
      "loss: 0.267118  [12832/60000]\n",
      "loss: 0.482417  [16032/60000]\n",
      "loss: 0.175631  [19232/60000]\n",
      "loss: 0.250015  [22432/60000]\n",
      "loss: 0.508028  [25632/60000]\n",
      "loss: 0.521141  [28832/60000]\n",
      "loss: 0.256521  [32032/60000]\n",
      "loss: 0.298513  [35232/60000]\n",
      "loss: 0.268106  [38432/60000]\n",
      "loss: 0.216152  [41632/60000]\n",
      "loss: 0.327513  [44832/60000]\n",
      "loss: 0.114184  [48032/60000]\n",
      "loss: 0.082669  [51232/60000]\n",
      "loss: 0.326298  [54432/60000]\n",
      "loss: 0.363356  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.255140 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:29:37.058372Z",
     "start_time": "2025-02-25T09:29:33.935656Z"
    }
   },
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "train_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:29:37.064678Z",
     "start_time": "2025-02-25T09:29:37.061374Z"
    }
   },
   "source": [
    "print(f'Neural network accuracy on train set: {train_acc:3.5}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.92525\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:29:39.296339Z",
     "start_time": "2025-02-25T09:29:37.139711Z"
    }
   },
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "test_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:29:39.307507Z",
     "start_time": "2025-02-25T09:29:39.304340Z"
    }
   },
   "source": [
    "print(f'Neural network accuracy on test set: {test_acc:3.5}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.9272\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:29:44.303395Z",
     "start_time": "2025-02-25T09:29:44.300396Z"
    }
   },
   "source": [
    "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
    "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Загрузите файл `hw07_data_dict.npy` (ссылка есть на странице с заданием) и запустите код ниже для генерации посылки. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:36:31.396595Z",
     "start_time": "2025-02-25T09:36:31.356597Z"
    }
   },
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "\n",
    "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "    \n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    'train': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])).numpy(),\n",
    "    'test': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['test'])).numpy()\n",
    "}\n",
    "\n",
    "np.save('submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
    "print('File saved to `submission_dict_hw07.npy`')\n",
    "# __________end of block__________"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], shape=(1000, 784), dtype=float32), 'test': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], shape=(1000, 784), dtype=float32)}\n",
      "{'train': array([0, 9, 0, 2, 8, 5, 9, 9, 4, 4, 0, 5, 7, 7, 0, 2, 4, 3, 5, 6, 7, 1,\n",
      "       4, 5, 5, 7, 7, 2, 2, 2, 7, 2, 0, 1, 8, 6, 9, 6, 9, 6, 9, 8, 0, 4,\n",
      "       4, 1, 9, 4, 9, 0, 3, 7, 7, 6, 1, 1, 3, 4, 0, 0, 3, 8, 0, 3, 1, 7,\n",
      "       2, 5, 3, 5, 4, 2, 3, 4, 0, 1, 2, 1, 3, 2, 0, 8, 1, 7, 0, 0, 4, 5,\n",
      "       0, 3, 1, 8, 4, 8, 2, 5, 9, 2, 9, 6, 3, 1, 5, 4, 3, 7, 0, 8, 6, 5,\n",
      "       8, 6, 2, 4, 4, 8, 1, 5, 3, 6, 0, 6, 6, 8, 9, 9, 6, 7, 7, 7, 9, 0,\n",
      "       6, 5, 7, 2, 0, 0, 8, 9, 7, 7, 9, 6, 8, 1, 3, 1, 5, 7, 1, 4, 4, 5,\n",
      "       4, 2, 3, 8, 2, 0, 0, 3, 1, 0, 6, 4, 3, 6, 4, 4, 6, 2, 1, 8, 4, 3,\n",
      "       7, 8, 8, 7, 9, 0, 3, 9, 9, 2, 3, 3, 4, 1, 1, 9, 6, 7, 8, 7, 6, 6,\n",
      "       9, 3, 3, 2, 8, 2, 5, 4, 2, 2, 6, 9, 3, 7, 0, 8, 9, 6, 7, 0, 7, 6,\n",
      "       1, 7, 1, 0, 9, 2, 2, 9, 3, 8, 0, 8, 6, 1, 1, 3, 2, 7, 9, 2, 0, 7,\n",
      "       4, 9, 9, 6, 5, 9, 4, 6, 9, 2, 9, 5, 5, 3, 7, 6, 4, 3, 4, 0, 9, 3,\n",
      "       3, 3, 8, 2, 6, 5, 1, 5, 8, 2, 5, 6, 0, 1, 1, 7, 6, 7, 5, 3, 1, 9,\n",
      "       4, 8, 0, 6, 2, 1, 2, 4, 0, 7, 8, 1, 6, 8, 0, 1, 2, 2, 1, 8, 9, 3,\n",
      "       1, 4, 9, 3, 8, 0, 0, 5, 5, 8, 3, 2, 8, 2, 1, 5, 4, 0, 4, 6, 5, 7,\n",
      "       5, 0, 3, 6, 9, 9, 1, 6, 7, 7, 6, 5, 8, 4, 6, 0, 3, 6, 8, 2, 5, 6,\n",
      "       7, 9, 1, 8, 6, 1, 4, 9, 2, 6, 7, 0, 5, 6, 7, 9, 1, 1, 6, 3, 9, 2,\n",
      "       3, 0, 7, 4, 1, 6, 2, 2, 1, 3, 0, 6, 2, 9, 3, 3, 7, 3, 4, 3, 5, 9,\n",
      "       1, 8, 7, 4, 1, 9, 4, 5, 5, 3, 9, 8, 7, 7, 7, 6, 9, 9, 3, 9, 8, 7,\n",
      "       3, 7, 1, 8, 4, 2, 7, 0, 1, 1, 0, 6, 3, 2, 8, 4, 1, 2, 7, 3, 9, 3,\n",
      "       9, 6, 1, 5, 7, 9, 1, 8, 1, 3, 9, 9, 3, 5, 3, 4, 0, 3, 6, 7, 6, 3,\n",
      "       9, 6, 0, 2, 9, 6, 5, 4, 2, 3, 9, 8, 8, 7, 0, 1, 7, 3, 3, 4, 2, 1,\n",
      "       5, 2, 8, 6, 0, 8, 5, 9, 3, 7, 6, 6, 8, 8, 7, 7, 6, 9, 0, 6, 9, 4,\n",
      "       6, 4, 5, 6, 8, 1, 4, 6, 4, 1, 0, 9, 2, 9, 4, 5, 7, 6, 3, 2, 2, 5,\n",
      "       1, 2, 8, 3, 6, 8, 2, 8, 1, 0, 8, 7, 1, 5, 1, 4, 6, 7, 6, 0, 3, 8,\n",
      "       7, 5, 5, 2, 4, 9, 9, 3, 5, 0, 3, 4, 8, 7, 7, 2, 0, 9, 7, 0, 7, 4,\n",
      "       0, 7, 7, 1, 7, 5, 0, 2, 5, 7, 0, 1, 2, 4, 9, 3, 8, 2, 7, 4, 8, 7,\n",
      "       6, 5, 7, 6, 9, 7, 7, 7, 2, 6, 0, 7, 9, 6, 2, 3, 5, 7, 4, 4, 8, 5,\n",
      "       2, 7, 8, 8, 4, 4, 5, 1, 6, 9, 3, 1, 2, 0, 9, 8, 5, 8, 8, 1, 2, 1,\n",
      "       9, 5, 1, 3, 2, 5, 7, 5, 0, 3, 6, 4, 2, 7, 0, 6, 0, 3, 3, 1, 0, 2,\n",
      "       2, 8, 2, 1, 8, 9, 5, 7, 6, 6, 2, 6, 6, 0, 1, 1, 7, 3, 1, 5, 1, 8,\n",
      "       3, 6, 3, 0, 5, 6, 7, 0, 5, 7, 3, 3, 1, 0, 8, 1, 3, 0, 7, 6, 4, 9,\n",
      "       3, 5, 9, 9, 6, 4, 7, 6, 2, 4, 1, 9, 8, 1, 7, 3, 1, 7, 0, 8, 6, 7,\n",
      "       4, 5, 7, 1, 9, 7, 7, 5, 7, 7, 9, 8, 6, 7, 3, 7, 0, 3, 8, 0, 9, 3,\n",
      "       9, 5, 4, 1, 7, 0, 6, 7, 5, 5, 5, 3, 2, 2, 6, 5, 9, 1, 1, 4, 8, 4,\n",
      "       2, 8, 2, 4, 6, 1, 2, 0, 7, 0, 3, 3, 2, 9, 9, 7, 4, 6, 8, 8, 9, 4,\n",
      "       9, 8, 4, 2, 0, 1, 0, 7, 5, 9, 7, 7, 9, 8, 6, 6, 7, 5, 8, 4, 3, 8,\n",
      "       5, 3, 9, 4, 4, 2, 3, 3, 0, 8, 7, 3, 3, 7, 5, 6, 5, 7, 1, 4, 7, 3,\n",
      "       3, 4, 2, 5, 3, 9, 9, 6, 2, 7, 8, 8, 7, 7, 7, 0, 9, 5, 6, 8, 4, 8,\n",
      "       3, 2, 7, 5, 5, 7, 6, 6, 0, 4, 4, 5, 6, 6, 8, 1, 6, 3, 4, 6, 1, 5,\n",
      "       6, 5, 7, 7, 5, 9, 1, 1, 4, 9, 3, 4, 6, 7, 3, 6, 7, 9, 8, 4, 3, 3,\n",
      "       1, 1, 7, 3, 8, 1, 2, 1, 0, 9, 5, 8, 4, 4, 1, 2, 6, 5, 9, 1, 4, 4,\n",
      "       0, 3, 4, 8, 1, 9, 8, 7, 7, 5, 6, 1, 8, 2, 5, 0, 5, 7, 2, 7, 2, 8,\n",
      "       5, 7, 3, 4, 9, 2, 8, 8, 7, 7, 9, 0, 8, 6, 1, 7, 8, 6, 0, 1, 4, 4,\n",
      "       2, 8, 9, 5, 9, 9, 4, 8, 8, 1, 6, 5, 8, 0, 7, 8, 4, 7, 5, 2, 0, 7,\n",
      "       7, 8, 2, 0, 3, 8, 7, 0, 9, 2]), 'test': array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
      "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
      "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3, 7, 4,\n",
      "       6, 4, 3, 0, 7, 0, 2, 7, 1, 7, 3, 7, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
      "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4,\n",
      "       8, 7, 3, 9, 7, 4, 4, 4, 9, 2, 5, 4, 7, 6, 4, 9, 0, 5, 8, 5, 6, 6,\n",
      "       5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1, 7, 1, 8, 2, 0, 9, 9, 9, 5, 5,\n",
      "       1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5, 1, 4, 4, 7, 2, 3, 2, 7,\n",
      "       1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1, 0, 9, 0, 3, 1, 6,\n",
      "       4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0, 3, 5, 5, 5,\n",
      "       7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 7, 7, 9, 2, 2, 4, 1, 5, 8,\n",
      "       8, 7, 2, 5, 0, 2, 4, 2, 4, 1, 9, 5, 7, 7, 2, 8, 2, 0, 8, 5, 7, 7,\n",
      "       9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2,\n",
      "       6, 4, 1, 5, 4, 2, 9, 2, 0, 4, 0, 0, 2, 8, 6, 7, 1, 2, 4, 0, 2, 7,\n",
      "       4, 3, 3, 0, 0, 5, 1, 9, 6, 5, 2, 5, 7, 7, 9, 3, 0, 4, 2, 0, 7, 1,\n",
      "       1, 2, 1, 5, 3, 3, 9, 7, 8, 6, 3, 4, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5,\n",
      "       0, 6, 1, 8, 5, 1, 7, 9, 4, 6, 7, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8,\n",
      "       8, 5, 4, 1, 1, 4, 0, 7, 3, 7, 6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5,\n",
      "       2, 5, 4, 4, 2, 8, 3, 8, 2, 4, 5, 0, 3, 1, 7, 7, 3, 7, 9, 7, 1, 9,\n",
      "       2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8, 4, 5, 9, 7, 8, 3, 7, 6,\n",
      "       0, 0, 3, 0, 8, 0, 6, 4, 8, 5, 3, 3, 2, 3, 9, 1, 2, 6, 8, 0, 5, 6,\n",
      "       6, 6, 7, 8, 8, 2, 2, 5, 8, 9, 6, 1, 8, 4, 1, 2, 8, 3, 1, 9, 7, 5,\n",
      "       4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 2, 9, 4, 0, 6, 3, 9, 3, 2, 1, 3,\n",
      "       1, 5, 6, 5, 7, 8, 2, 2, 6, 8, 2, 6, 5, 4, 8, 9, 7, 1, 3, 0, 3, 8,\n",
      "       3, 1, 9, 6, 4, 4, 6, 4, 1, 1, 8, 2, 5, 4, 8, 3, 4, 0, 0, 2, 3, 2,\n",
      "       7, 1, 0, 6, 7, 4, 4, 7, 9, 6, 9, 0, 9, 8, 0, 9, 6, 0, 6, 5, 5, 4,\n",
      "       8, 3, 3, 9, 3, 3, 2, 7, 8, 0, 2, 2, 1, 7, 0, 6, 5, 4, 3, 3, 0, 9,\n",
      "       6, 3, 8, 0, 9, 9, 6, 8, 6, 8, 5, 7, 8, 6, 0, 2, 4, 0, 2, 8, 3, 1,\n",
      "       9, 7, 5, 8, 0, 8, 4, 6, 2, 6, 7, 9, 9, 6, 9, 8, 2, 2, 9, 2, 7, 3,\n",
      "       5, 9, 1, 8, 0, 2, 0, 5, 2, 1, 3, 7, 6, 7, 1, 2, 5, 8, 0, 3, 7, 9,\n",
      "       4, 0, 9, 1, 8, 6, 7, 7, 4, 3, 4, 9, 1, 9, 5, 1, 7, 3, 9, 7, 6, 9,\n",
      "       1, 3, 3, 8, 3, 3, 6, 7, 2, 4, 5, 8, 5, 1, 1, 4, 4, 3, 1, 0, 7, 7,\n",
      "       0, 7, 9, 9, 4, 8, 5, 5, 4, 0, 8, 2, 1, 6, 8, 4, 8, 0, 4, 0, 6, 1,\n",
      "       7, 3, 8, 6, 7, 2, 6, 9, 3, 1, 4, 6, 8, 5, 9, 2, 0, 6, 2, 1, 7, 3,\n",
      "       4, 1, 0, 5, 4, 3, 1, 1, 7, 4, 9, 9, 9, 8, 4, 0, 2, 4, 5, 1, 1, 6,\n",
      "       4, 7, 1, 9, 4, 2, 4, 1, 5, 5, 3, 8, 3, 1, 4, 5, 6, 8, 9, 4, 1, 9,\n",
      "       3, 8, 0, 3, 2, 5, 1, 2, 8, 3, 4, 4, 0, 8, 8, 3, 3, 1, 7, 3, 5, 9,\n",
      "       6, 3, 2, 6, 1, 3, 6, 0, 7, 2, 1, 7, 1, 4, 2, 4, 2, 1, 7, 9, 6, 1,\n",
      "       1, 2, 4, 3, 1, 7, 7, 4, 7, 0, 7, 3, 1, 3, 1, 0, 7, 7, 0, 3, 5, 3,\n",
      "       2, 7, 6, 6, 9, 2, 8, 3, 5, 2, 2, 5, 6, 0, 8, 2, 9, 2, 8, 8, 8, 8,\n",
      "       7, 9, 7, 3, 0, 6, 6, 3, 2, 1, 3, 2, 2, 9, 3, 0, 0, 5, 2, 8, 1, 4,\n",
      "       4, 6, 0, 2, 9, 1, 4, 7, 4, 7, 3, 9, 8, 8, 4, 7, 1, 2, 1, 2, 2, 3,\n",
      "       7, 3, 2, 3, 9, 1, 7, 4, 0, 3, 5, 5, 8, 6, 5, 0, 6, 7, 6, 6, 3, 2,\n",
      "       7, 9, 1, 1, 2, 5, 6, 4, 9, 5, 2, 3, 3, 9, 7, 8, 9, 1, 1, 0, 9, 1,\n",
      "       4, 4, 5, 4, 0, 6, 2, 2, 3, 1, 5, 1, 2, 0, 8, 8, 1, 2, 6, 7, 1, 6,\n",
      "       2, 3, 9, 0, 1, 2, 2, 0, 8, 9])}\n",
      "File saved to `submission_dict_hw03.npy`\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:53:36.607118Z",
     "start_time": "2025-02-25T09:53:36.587208Z"
    }
   },
   "cell_type": "code",
   "source": "np.load('submission_dict_hw07_01.npy', allow_pickle=True)",
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Failed to interpret file 'submission_dict_hw07_01.npy' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnpicklingError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\ya_ml_training_1\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:491\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 491\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpickle_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mUnpicklingError\u001B[0m: invalid load key, '\\xe2'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mUnpicklingError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[60], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msubmission_dict_hw07_01.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_pickle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ya_ml_training_1\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:493\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    491\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mload(fid, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_kwargs)\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 493\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(\n\u001B[0;32m    494\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to interpret file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m as a pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mUnpicklingError\u001B[0m: Failed to interpret file 'submission_dict_hw07_01.npy' as a pickle"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:02:23.903157Z",
     "start_time": "2025-02-25T11:02:23.886200Z"
    }
   },
   "cell_type": "code",
   "source": "get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])).numpy().dtype",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
